<body>
   <article xmlns:xlink="http://www.w3.org/1999/xlink">
      <h1> INTRODUCTION</h1>
      <p>The impact of university ranking has begun to be considered since the appearance of international rankings in 2003/2004 (league
         tables in the United Kingdom). From then on, several rankings have been published frequently and also discussed in the media
         to inform and influence public opinion, student decisions, university strategies and government policies. This combination
         of results, media information and public debate produces effects that deserve to be considered. 
      </p>
      <p> Informative and academic interest in university rankings at the international level has increased significantly. For example,
         a search of the term ‘university ranking/s’ in the Web of Science database (WOS), reveals that almost 2500 academic articles
         have been published in English since 2000. Considering the year of creation of the main international rankings (2003/2004),
         the number of relevant scientific articles has increased twelvefold in ten years: from 30 articles in 2004 to 299 in 2018.
         This growing presence of rankings in the main databases of the international scientific literature reveals that the publication
         of university rankings is of undoubted interest to the university community, as well as to society in general. This popularity
         of the rankings is fuelled by university managers and policy makers, as well as their recipients (<a href="#" id="bib18">Fowles, Frederickson, &amp; Koppell, 2016a</a>; <a href="#" id="bib36">Millot, 2015</a>; <a href="#" id="bib38">O'Connell, 2015</a>). However, these articles contain criticisms by scientists and experts, especially regarding methodological matters. This
         attention increases their visibility in political decision-making, and attracts even more attention in the media (<a href="#" id="bib10">Daraio, Bonaccorsi, &amp; Simar, 2015</a>). In the same way, higher-status universities’ rankings contribute to disseminating this information and to establishing
         mechanisms and structures of corporate governance between them, creating clusters of well-placed universities, as part of
         a marketing strategy. Summing up, the more visible these rankings are (even when regarded negatively), the more they are used.
         And they are increasingly visible. 
      </p>
      <p>This ranking information is used by universities for different objectives: increasing competitiveness, making comparisons
         or planning strategic education policy decisions at various levels: global, institutional and national (<a href="#" id="bib25">Hazelkorn, Loukkola, &amp; Zhang, 2013</a>; <a href="#" id="bib29">Kehm, 2014</a>; <a href="#" id="bib44">Vladimirovich &amp; Nikolayevna, 2013</a>). It has been seen that one of the requirements to be taken into account in the development of some university procedures
         and decision-making is for the university to attain the highest possible position in the rankings. This has approach has even
         been backed by national policies. For instance, ranking has been observed to be a factor in the recognition of university
         degrees or in the eligibility of institutions for the establishment of bilateral agreements (<a href="#" id="bib16">European University Association, 2013</a>). Specifically, rankings have been used in fellowship allocation policies for students in contexts of high social inequality
         and stratified educational systems, increasing the differences between students who are in a privileged situation compared
         to those who have fewer educational opportunities (<a href="#" id="bib42">Perez Mejia, Chiappa, &amp; Guzmán-Valenzuela, 2018</a>). 
      </p>
      <p>Rankings also play a decisive role in economic issues, such as the price of tuition fees (<a href="#" id="bib43">Tofallis, 2012</a>). There is evidence that university rankings are used as an indicator for policy-making, not only as a measurement, but also
         merely as a goal in itself. For instance, the goal is to have a percentage of the national universities within the top 100
         in one specific ranking. However, if the ranking is the goal in itself, what are the objectives of the rankings? We can define
         the publication of a list of the best universities, establishing comparisons between them and showing the best options for
         students’ studies, as the main objective of university rankings. In this sense, the results published in these lists affect
         both students and managers, modifying their behaviour patterns, especially in selecting the best university to study at and
         in improving or maintaining the position within these lists. 
      </p>
      <p>Taking into account the impact of the publications of the rankings in the media, some of the ways in which universities can
         develop specific strategies for reaching a higher position in the rankings are, for example, promoting student mobility, adjusting
         the numbers of teachers/students and attracting prestigious visiting professors from international universities. The management
         of universities is also affected by these rankings, especially in terms of the need for possible resources to reach top positions
         on the lists and the remuneration of high-ranking university officials (<a href="#" id="bib16">European University Association, 2013</a>). 
      </p>
      <p>Clearly, the rankings are affecting the decisions made by universities, administrations and even students. These decisions
         reinforce the dominance of universities that appear in the highest positions, setting them up as models for the others to
         follow, whereas the other universities (the majority) receive constant annual criticism instead of help to improve. Based
         on this, what then are these rankings that provoke this pressure on or ‘harassment’ of most universities? 
      </p>
      <p>Taking into account the existence and impact on the media of these rankings, the purpose of this article is to delve deeper
         into their negative impact, the options available to a university and the strategies that a university can adopt, whether
         it is considered world-class or not. In this paper we will focus our analysis on these two research questions: 
      </p>
      <ol>
         <li>Firstly, due to the ranking system’s weaknesses and threats, it does not seem to be a useful scenario for universities overall,
            but what strengths and opportunities exist? 
         </li>
         <li>Secondly, what are the consequences of this differentiation of universities and what are the universities’ </li>
      </ol>
      <p>Therefore, the objectives of this paper are (1) synthesize the characteristics of the rankings that cause pressure on universities
         and (2) analyse and define possible strategies for action that can be carried out in the real context of the strategic policy
         of universities. 
      </p>
      <p>The thesis of this study is that the rankings establish inadequate objectives for most universities. Therefore, this study
         presents a complete synthesis of the arguments against the use of the rankings as an accountability tool and provides arguments
         for managers to defend themselves against criticism of the poor performance of their institutions resulting from the publication
         of these rankings. 
      </p>
      <h1> METHOD</h1>
      <p> This research is based on a theoretical-methodological approach through an inductive content analysis of previous research
         studies on international rankings in the university field. In this study, a narrative literature review has been carried out
         of scientific articles published in English between 2012 and 2019 from the main databases: Web of Science, Scopus and ERIC,
         with these keywords in their titles: international ranking* AND universit*. Also, recent reports from the European University
         Association have been analysed. This bottom-up approach develops constructs and theories aligned with the literature (<a href="#" id="bib23">Gupta, Shaheen, &amp; Reddy, 2018</a>) and is aimed at identifying, analysing, evaluating and interpreting the body of knowledge on a specific topic; in this case,
         international university rankings. 
      </p>
      <p>The rankings can be classified as national and international rankings depending on their scope of action. Firstly, national
         rankings are more coherent, considering that the characteristics of universities within the same country are more similar,
         such as type of students, national politics, etc., and are based on a wide range of comparative indicators (retention rates,
         graduates' salaries, etc.). Nevertheless, they have limited media impact. Secondly, international rankings, which are based
         on a few available indicators and expert opinions, have the greatest media impact on society and on public policies, especially
         at the national level, particularly in those countries where national rankings are not a tradition. Although national rankings
         are more accurate given the diversity of institutions, policy design and global debates are based mainly on the international
         ones. This paper is focused on the three main international rankings, since these have greater visibility and weight in debates
         and public policies (<a href="#" id="bib44">Vladimirovich &amp; Nikolayevna, 2013</a>): 
      </p>
      <ul>
         <li>QS - World University Ranking.</li>
         <li>THE - Times Higher Education World University Ranking.</li>
         <li>ARWU - Academic Ranking of World Universities, also known as Shanghai Ranking.</li>
      </ul>
      <p>These three ranking are the most used for representing the concept of international university rankings. This study indicates
         the common aspects of these three international rankings, emphasizing and highlighting the specifications of each of them
         as, although they have a common weight and sum methodology, it should be taken into account that they use different weights
         and indicators. 
      </p>
      <p>The coding process of the summative content analysis is based on the main categories of a SWOT analysis: strengths, weaknesses,
         opportunities, and threats, identifying and describing the differences and similarities of these three rankings, as a critical
         discourse analysis and a classical strategic planning procedure, in order to provide universities with arguments for discussion.
         This content analysis has been used to identify common features of and differences between the three rankings, avoiding any
         generalisation. This coding facilitates analysis of complex situations and environments such as the university ranking system,
         identifying their internal and external key factors in order to adopt effective strategies and decision-making, as is pointed
         out in the conclusions of this paper. 
      </p>
      <p>Finally, based on this analysis, the main implications of the university rankings for institutional strategies are described,
         focusing on (1) the stratified system of institutions (the world-class universities) and (2) on the institutional mission
         as a higher education institution. 
      </p>
      <h1> RESULTS</h1>
      <h2> SWOT framework for international rankings</h2>
      <h3>Strengths </h3>
      <p>The information provided by rankings is useful for some recipients (<a href="#" id="bib16">European University Association, 2013</a>; <a href="#" id="bib22">Goglio, 2016</a>). Firstly, for potential students and their families, since the rankings help them to choose an appropriate university based
         on their needs and interests, whether within their country or abroad. Secondly, for governments, because the information allows
         them to make decisions about possible political changes and justify to the general public the need for possible reforms in
         universities (<a href="#" id="bib14">Erkkilä, 2014</a>). Thirdly, for society, as being more informed about decisions taken at the political level and having more reliable data
         on higher education promotes institutional transparency (<a href="#" id="bib5">Bengoetxea &amp; Buela-Casal, 2013</a>; <a href="#" id="bib36">Millot, 2015</a>). Lastly, for the universities themselves. The results of the rankings allow these institutions to carry out internal analyses,
         to reflect on how to measure international success, improve institutional practices and make comparisons between universities.
      </p>
      <h3>Opportunities</h3>
      <p>Rankings contribute to the rapid globalisation and internalisation of higher education, the increase in the number of university
         students and their mobility, the involvement and participation of the institutions in national and international debates,
         and the increase in collaboration between universities and the community (<a href="#" id="bib40">Pavel, 2015</a>). These aspects are reflected in some indicators under international outlook and industry income criteria such as ‘international
         faculty and student ratios’ or ‘knowledge-transfer activities’. 
      </p>
      <p>The publication of rankings also forces universities to change their organisation and behaviour (<a href="#" id="bib22">Goglio, 2016</a>). Furthermore, in some cases, they could provide a useful comparison of universities that are similar in size, age or field
         of specialisation (<a href="#" id="bib39">Olcay &amp; Bulu, 2017</a>), avoiding subjectivity regarding the attributes of the universities (<a href="#" id="bib7">Bougnol &amp; Dulá, 2015</a>). Finally, as regards students, rankings providing information separately on all quality dimensions, rather than publishing
         international classifications as a whole, could be useful to support a well-informed university choice and, as a consequence,
         lead to a reduction in dropout rates, an increase in human capital production and even an improvement in general welfare (<a href="#" id="bib26">Horstschräer, 2012</a>). 
      </p>
      <h3>Weaknesses</h3>
      <p>Most of the weaknesses are related to serious methodological aspects. However, the different actors involved in university
         classifications may not be fully aware of these difficulties, or the complexity of the existing processes and criteria for
         the development of rankings, even though they show great interest in knowing what position a university is in (<a href="#" id="bib2">Al-Juboori, Su, &amp; Ko, 2012</a>). Each ranking system uses different sources of information, some of which are of questionable transparency (<a href="#" id="bib3">Anowar et al., 2015</a>), and evaluates universities according to its own criteria using indicators that do not cover all the activities that a university
         can develop, strongly focusing on the institutions’ investigations (<a href="#" id="bib20">Ganga-Contreras, San Martin, &amp; Viancos, 2019</a>). For example, most ranking systems are based essentially on research activities (<a href="#" id="bib34">Margison, 2014</a>), including indicators such as research productivity, research income or papers published in Nature and Science; leaving
         aside the other functions of the universities (Figure 1). This is especially notable in the case of the ARWU, where all its
         indicators refer to the quality of research activity. The indicators related to research activity may also be focused on certain
         elements to the exclusion of others, such as the merits obtained in domains of knowledge such as arts, humanities and even
         social sciences, or those publications of great impact that are not articles in scientific journals. 
      </p>
      <figure><img src="475-2715-1-sp.png" alt=""/><figcaption><strong>Figure 1. </strong>Classification by area of the indicators used in the QS, THE and ARWU rankings 
         </figcaption>
      </figure>
      <p></p>
      <p>In addition to the research indicators, the evaluation of the quality of university activities is based on reputation surveys
         with a large weight in the overall score in two systems (70 per cent in the QS and 63 per cent in the THE). In these surveys,
         academics are asked to identify the institutions in which they believe that their own field of specialisation is being best
         developed. But, how do they retrieve this information about prestige of universities? For example, the THE survey in 2017
         asked for the academics ‘views on (1) The best research universities in the world, (2) the best teaching universities in the
         world, (3) additional best research universities within your country, (4) additional best teaching universities within your
         country’. The type of question for these reputation indicators is ‘choose up to 15 institutions in the world (in any order)
         that you regard as producing the best teaching within your subject area’. For the purposes of this paper only three ideas
         need to be emphasized. Firstly, the question is too open in that, whatever the answer is, the most well-known universities
         will be mentioned by most of the respondents. Secondly, the lack of sampling procedure description used for the survey implies
         a high risk of bias. Finally, there are serious doubts about the expertise of the ‘rankers' (<a href="#" id="bib31">Lim, 2017</a>). 
      </p>
      <p>Therefore, the description of universities on the basis of their research activity does not seem to be entirely adequate,
         but comparisons based on reputation and not on the merits of the action only benefit the institutions that are already leaders
         in the rankings and fail to promote the improvement of the rest (<a href="#" id="bib34">Margison, 2014</a>). The use of soft data (that is, qualitative data based on opinions, ideas, knowledge, experience) and the weighting systems
         used can favour some well-known universities in developed countries (<a href="#" id="bib3">Anowar et al., 2015</a>; <a href="#" id="bib39">Olcay &amp; Bulu, 2017</a>). 
      </p>
      <p>There is evidence that all of the rankings have weaknesses and none of the existing rankings is perfect. This fact is not
         to suggest rejecting all comparison and public information about them. It is a matter of paying attention to criticism and
         periodically revising the classifications in order to adapt them to the reality of universities (<a href="#" id="bib28">Kaycheng, 2015</a>). Anther option is to elaborate more useful classifications adapted to the reality of universities and their missions, using
         concrete measures and reliable data from universities that are not based on opinion surveys (<a href="#" id="bib34">Margison, 2014</a>). 
      </p>
      <p>Despite all of these weaknesses, the results are considered solid information to the point of being considered an unavoidable
         indicator for national and institutional higher education policies. 
      </p>
      <h3>Threats</h3>
      <p>The first threat is the confirmation that many important aspects related to the quality of university education (<a href="#" id="bib24">Hazelkorn, 2008</a>) are not measured in relation to other essential values in higher education (<a href="#" id="bib6">Blanco-Ramírez &amp; Berger, 2014</a>) and society, such as human capital formation (<a href="#" id="bib44">Vladimirovich &amp; Nikolayevna, 2013</a>). This implies a risk of loss of interest in those dimensions that are not measured. 
      </p>
      <p>The second threat is doubt about what is really being measured: the reputation or the performance of universities. While the
         first one is a social construct based on perceptions, the second one, that is real university activity, is measured through
         peer review in which there is knowledge of who is the best. This performance review of the universities and the resulting
         classifications are carried out by academics and refers to specific fields of research, but not to the institution as a whole
         (<a href="#" id="bib29">Kehm, 2014</a>). 
      </p>
      <p>The last threat is the periodical publication of the rankings, which provokes concern in many universities about the systematic
         collection of data that allows universities to rise in the rankings year by year, converting the means (measurement) into
         the goal (to be measured). However, after many years of reports, we already know that results are very stable: the correlation
         (r Pearson) of results from one year to the next is up to 0,96 (<a href="#" id="bib19">Fowles, Frederickson, &amp; Koppell, 2016b</a>). Thus, what we have has been a persistent system of exalting the top universities and of disparaging the rest. As a consequence,
         many university leaders spend their time restoring internal morale and public confidence (<a href="#" id="bib24">Hazelkorn, 2008</a>) and many ministers use the results as an issue of national pride and an attractive selling point to encourage inward investment
         (Hazelkorn,<a href="#" id="bib24">2008</a>, p. 21). Both activities seem to be a waste of time and, in many cases, money. 
      </p>
      <p>In short, the weaknesses and threats of the rankings cast a significant shadow over their strengths and opportunities. Given
         that, then, what are their effects upon, and the options available to a university? What strategies can a university adopt,
         whether it is world-class or not? A few (world-class) universities emphasise their good results, and many others not appearing
         in the top groups are pressured every year to answer for their poor classification. 
      </p>
      <h2>Implications of international rankings</h2>
      <h3>For world-class universities </h3>
      <p>One of the consequences of the impact of the rankings is the establishment of a stratified system of universities that causes
         their commercialisation (<a href="#" id="bib14">Erkkilä, 2014</a>), especially in the case of the so-called world-class universities. The main advantages for these universities are that it
         is relatively easy for them to recruit partners and funders into their collaborative activities (including economic support
         from government and business) and to attract more and better students (<a href="#" id="bib26">Horstschräer, 2012</a>) and professors, especially those from other countries since, in some cases, this is one of the indicators included in the
         rankings (<a href="#" id="bib44">Vladimirovich &amp; Nikolayevna, 2013</a>), such as with ‘international to domestic student/staff ratios and international collaboration’. 
      </p>
      <p>This effect makes the recognition and publication of ranking information a powerful marketing tool for universities, especially
         for world-class universities (<a href="#" id="bib32">Locke, 2014</a>), as well as providing a way to attract more resources: students, financing, projects, etc (<a href="#" id="bib9">Daniela, Casani, García-Zorita, Efraín-García, &amp; Sanz-Casado, 2012</a>). However, there are other factors that also promote the presence of certain universities in the top positions, such as being
         in rich and more democratic countries but, above all, being transparent. This aspect should be considered in decision-making
         by governments since, if they increase transparency, they increase the likelihood that their universities will be considered
         as world-class (<a href="#" id="bib27">Jabnoun, 2015</a>). Governments are increasingly adopting strategies for providing access to university information as a means to ensure academic
         quality due to the competitiveness that university rankings generate. These classifications are justified as an efficient
         means for providing information to potential clients (students) as well as to the institutions and decision makers in areas
         requiring improvement (<a href="#" id="bib11">Dill &amp; Soo, 2005</a>). 
      </p>
      <p>For these world-class universities, the rankings represent an opportunity to stand out from the rest, with consequences for
         their marketing strategies and alliances. This does not seem to affect their ordinary activity, especially when they are strongly
         research-oriented (<a href="#" id="bib13">Elken, Hovdhaugen, &amp; Stensaker, 2016</a>), but they take advantage of the image that the rankings provide, such as the ‘quality’ label. 
      </p>
      <h3>For other universities</h3>
      <p>The rest of the universities are, by definition, most universities. These are excluded not only from the top positions, but
         also from the reputation race itself. Their mission is not (and is never going to be) under the umbrella of the areas and
         indicators that are evaluated in the main rankings. This is particularly true in the case of teaching-oriented universities,
         those that concentrate university degrees on the arts or humanities or those concentrated on local or national social needs.
         Rankings do not respond to all universities, since only a small percentage (about 3-5 per cent) belong to the group considered
         world-class universities (<a href="#" id="bib40">Pavel, 2015</a>; <a href="#" id="bib45">Wilson, 2011</a>). Neither do the rankings respond to the complexity of the entire higher education system (<a href="#" id="bib36">Millot, 2015</a>), nor can we stop using them or avoid their consequences. 
      </p>
      <p>As a result, it is impossible for most universities (95%) to meet most of the requirements and criteria of the international
         rankings. This is made more difficult when they do not cover many areas that are within the mission of a university (<a href="#" id="bib3">Anowar et al., 2015</a>; <a href="#" id="bib40">Pavel, 2015</a>). If a university does not have the mission of being internationally excellent with a high level of research, it cannot be
         well reflected in the current rankings, and must resign itself to not belonging to the world’s top research universities.
         However, this should not be frustrating, whereas not fulfilling its mission as a university should be. The accessibility and
         simplicity of the data published in the rankings distracts attention from universities’ moral and political objectives, while
         its arbitrariness creates an impression that what is of merit can be hierarchically ordered and incontrovertibly judged (<a href="#" id="bib33">Lynch, 2015</a>). This implies a reactive measure in which the object of study can be modified due to the thing that one is trying to measure
         (<a href="#" id="bib15">Espeland &amp; Sauder, 2007</a>). 
      </p>
      <p>Nevertheless, there are other legitimate higher education values to be exalted. For instance, in a study carried out by (<a href="#" id="bib41">Pérez-Díaz &amp; Rodríguez, 2014</a>), students, professors and administrative staff considered that the highest priority of universities should be to train good
         professionals (78 per cent) and then carry out research tasks, knowledge development and innovation (65 per cent).<a href="#" id="bib35">Mengual-Andrés (2013)</a> focuses on the need of the universities to take an active role in the formal accreditation and validation of informal learning.
         In addition, within the so-called Third Mission (<a href="#" id="bib37">Mora, Ferreira, Vidal, &amp; Vieira, 2015</a>), universities must contribute to the economic and social development of the region in which the university is located (45
         per cent), train new citizens (35 per cent) and contribute to the reduction of social inequalitiy (34 per cent). Going further,
         we could look at the connection between the training given to students at universities and the achievement of life goals by
         these students (<a href="#" id="bib21">García-Aracil, Gabaldón, Mora, &amp; Vila, 2007</a>) as the main quality indicator. 
      </p>
      <p>In short, there are two options. If universities agree with the methodology and the mission behind ranking indicators, they
         should only continue to strive for the best results in the areas evaluated (<a href="#" id="bib36">Millot, 2015</a>); if not, they must define and defend their own mission and admit that the top rankings do not respond to all of the needs
         of the stakeholders and do not consider methodologically either the variety of missions of each university or the diversity
         of existing institutions (<a href="#" id="bib5">Bengoetxea &amp; Buela-Casal, 2013</a>). Therefore, universities may decide to adopt the values that the rankings establish, or to focus on the mission defined
         by the main stakeholders in each institution, giving up using the results of the rankings as a key aspect in their decisions
         and policies (<a href="#" id="bib12">Doğan &amp; Al, 2019</a>). In this second option, it is essential that all the mentioned stakeholders assume the consequences of it. This means that
         these institutions should not need to spend time and money explaining the same negative results every year. Instead, they
         should be aware (and proud) that their institution has a lot of goals to reach beyond the goals measured by the rankings and
         closer to the service they can give to society. Universities should not worry unduly about the fact that rankings are present
         and their ranking results, especially if institutions and staff have tried to do their academic work in the best possible
         way (<a href="#" id="bib3">Aw &amp; Sharif, 2011</a>). 
      </p>
      <h1> DISCUSSION AND CONCLUSIONS</h1>
      <p> It seems that international rankings are here to stay and there is enough information on how they are constructed and who
         is responsible for it. Besides the criticisms and negative opinions they generate, these rankings have a significant international
         impact on universities in terms of organisation, practice and structure (<a href="#" id="bib30">Lauder, Sari, Suwartha, &amp; Tjahjono, 2015</a>). In this study, the characteristics of the rankings that cause universities to be under pressure and the possible strategies
         for action that can be carried out in the real context were analysed. 
      </p>
      <p>International ranking is an issue for less than 5% of universities and, for them, it is mainly an issue of marketing and business,
         connected with international prestige. Moreover, apart from that, we know that 95% of universities spend resources every year
         answering questions about their ‘bad results’ and their future strategies to improve in the rankings. After 10 years of international
         rankings, neither have relevant changes been taking place nor will they take place in the top 100 or 200 places, and, if there
         are changes in the rest of the list, those changes neither have been nor will be relevant to the impact areas we have mentioned.
         New editions do not provide any new relevant information. 
      </p>
      <p>Nevertheless, the competitiveness of these rankings can allow universities to attract students, raise funds for their scientific
         activities and promote the awarding of prizes to teachers and students as well as contributions to significant discoveries
         (<a href="#" id="bib17"> Fernández Tuesta, Garcia-Zorita, Rosario, &amp; Sanz-Casado, 2019</a>), but the international rankings are harassing or bullying 95% of universities with the same results year after year. These
         institutions cannot spend time and money explaining the same results every year when some results are obvious and some are
         nonsense. These rankings cannot make university academics, staff and managers feel bad for what they are not, and what they
         are not commissioned to be. Besides that, these rankings are used as a reference for crucial decision-making (<a href="#" id="bib1">Aïssaoui &amp; Geringer, 2018</a>), to the point that the university’s autonomy, at the service of the needs of society, would be weakened in favour of private
         agents not legitimised to establish the mission of higher education institutions. Aspects related to the values, purposes
         and policies of higher education are being set aside in the designing of these rankings (<a href="#" id="bib33">Lynch, 2015</a>). 
      </p>
      <p>In order to face this reality, universities must keep to their pathways despite media pressures, consider whether they are
         close to their conception of university quality and understand the limitations of these classifications. It may happen that
         these universities are tempted to change their vision, with the consequent loss of their differential character, if policymakers
         use the rankings to set priorities (<a href="#" id="bib8">Climent, Michavila, &amp; Ripolles, 2013</a>). 
      </p>
      <p>As we have shown, these classifications are used as a main reference even though there is enough evidence to question their
         validity and reliability. A rational use of other sources of information, such as assessment and management tools in education,
         can be truly useful in analysing the weaknesses and strengths of the institution, as well as being part of the discussion
         in strategic decision-making and academic quality (<a href="#" id="bib5">Bengoetxea &amp; Buela-Casal, 2013</a>). 
      </p>
      <p>It could be concluded that we must spend time and money creating better rankings which overcome their well-known weaknesses,
         but there are not enough resources to find a way to compare on international bases all the dimensions of these complex institutions.
         Even so, there is no need for such a comparison. The problem is not the measurement tool, but the interest in what is meant
         to be measured. As a consequence, universities must consider whether they accept the rankings’ idea of university quality
         or keep to their differential character despite media pressures. We have tried to provide evidence that universities should
         concentrate on their mission and must provide valid and reliable information to all stakeholders about the level of achievement
         of their goals. Let us allow these universities to follow their mission and explain their achievements instead of wasting
         time and money explaining their rank in every ranking published yearly. 
      </p>
      <p></p>
      <h1>REFERENCES</h1>
      <ol id="references">
         <li>Aïssaoui, R., &amp; Geringer, M. J. (2018). International business research output and rankings of Asia-Pacific universities:
            A 40-year time-series analysis. <i>Asia Pacific Journal of Management</i>, <i>35</i>(4), 993–1023. <a href="https://doi.org/10.1007/s10490-017-9536-x">https://doi.org/10.1007/s10490-017-9536-x</a>. 
         </li>
         <li>Al-Juboori, A. F. M. A., Su, D., &amp; Ko, F. (2012). University ranking and evaluation: Trend and existing approaches. <i>International Journal of Advancements in Computing Technology</i>, <i>4</i>(5), 10–16. <a href="https://doi.org/10.4156/ijact.vol4.issue5.2">https://doi.org/10.4156/ijact.vol4.issue5.2</a>. 
         </li>
         <li>Anowar, F., Helal, M. A., Afroj, S., Sultana, S., Sarker, F., &amp; Mamun, K. A. (2015). 
         A Critical Review on World University Ranking in Terms of Top Four Ranking Systems.
         In K. Elleithy, &amp; T Sobh (Eds.), 
          <i>New Trends in Networking, Computing, E-learning, Systems Sciences, and Engineering</i>
          (pp. 559–566).  Switzerland: Springer International Publishing.  <a href="https://doi.org/10.1007/978-3-319-06764-3_72">https://doi.org/10.1007/978-3-319-06764-3_72</a>. 
         </li>
         <li>Aw, T. C., &amp; Sharif, A. (2011). Angst over university rankings. <i>Journal of Health and Translational Medicine</i>, <i>14</i>(1), 1–5. <a href="https://doi.org/10.22452/jummec.vol14no1.1">https://doi.org/10.22452/jummec.vol14no1.1</a>. 
         </li>

         <li>Bengoetxea, E., &amp; Buela-Casal, G. (2013). The new multidimensional and user-driven higher education ranking concept of the
            European Union. <i>International Journal of Clinical and Health Psychology</i>, <i>13</i>(1), 67–73. <a href="https://doi.org/10.1016/s1697-2600(13)70009-7">https://doi.org/10.1016/s1697-2600(13)70009-7</a>. 
         </li>
         <li>Blanco-Ramírez, G., &amp; Berger, J. B. (2014). Rankings, accreditation, and the international quest for quality. <i>Quality Assurance in Education</i>, <i>22</i>(1), 88–104. <a href="https://doi.org/10.1108/qae-07-2013-0031">https://doi.org/10.1108/qae-07-2013-0031</a>. 
         </li>
         <li>Bougnol, M.L, &amp; Dulá, J. H. (2015). Technical pitfalls in university rankings. <i>Higher Education</i>, <i>69</i>, 859–866. <a href="https://doi.org/10.1007/s10734-014-9809-y">https://doi.org/10.1007/s10734-014-9809-y</a>. 
         </li>
         <li>Climent, V., Michavila, F., &amp; Ripolles, M. (2013). <i>Los rankings universitarios, mitos y realidades [University rankings, myths and realities]</i>. Madrid:  Tecnos. 
         </li>
         <li>Daniela, D., Casani, F., García-Zorita, C., Efraín-García, P., &amp; Sanz-Casado, E. (2012). Visibility in international rankings.
            Strategies for enhancing the competitiveness of Spanish universities. <i>Scientometrics</i>, <i>93</i>(3), 949–966. <a href="https://dx.doi.org/10.1007/s11192-012-0749-y">https://dx.doi.org/10.1007/s11192-012-0749-y</a>. 
         </li>
         <li>Daraio, C., Bonaccorsi, A., &amp; Simar, L. (2015). Rankings and university performance: A conditional multidimensional approach.
            <i>European Journal of Operational Research</i>, <i>244</i>(3), 918–930. <a href="https://dx.doi.org/10.1016/j.ejor.2015.02.005">https://dx.doi.org/10.1016/j.ejor.2015.02.005</a>. 
         </li>
         <li>Dill, David D., &amp; Soo, M. (2005). Academic quality, league tables, and public policy: A cross-national analysis of university
            ranking systems. <i>Higher Education</i>, <i>49</i>(4), 495–533. <a href="https://dx.doi.org/10.1007/s10734-004-1746-8">https://dx.doi.org/10.1007/s10734-004-1746-8</a>. 
         </li>
         <li>Doğan, G., &amp; Al, U. (2019). Is it possible to rank universities using fewer indicators? A study on five international university
            rankings. <i>Aslib Journal of Information Management</i>, <i>71</i>(1), 18–37. <a href="https://dx.doi.org/10.1108/ajim-05-2018-0118">https://dx.doi.org/10.1108/ajim-05-2018-0118</a>. 
         </li>
         <li>Elken, M., Hovdhaugen, E., &amp; Stensaker, B. (2016). Global rankings in the Nordic region: challenging the identity of research-intensive
            universities? <i>Higher Education</i>, <i>72</i>, 781–795. <a href="https://dx.doi.org/10.1007/s10734-015-9975-6">https://dx.doi.org/10.1007/s10734-015-9975-6</a>. 
         </li>
         <li>Erkkilä, T. (2014). Global University Rankings, Transnational Policy Discourse and Higher Education in Europe. <i>European Journal of Education</i>, <i>49</i>(1), 91–101. <a href="https://doi.org/10.1111/ejed.12063">https://doi.org/10.1111/ejed.12063</a>. 
         </li>
         <li>Espeland, W. M., &amp; Sauder, M. (2007). Rankings and Reactivity: How Public Measures Recreate Social Worlds. <i>American Journal of Sociology</i>, <i>113</i>(1), 1–40. <a href="https://doi.org/10.1086/517897">https://doi.org/10.1086/517897</a>. 
         </li>
         <li>European University Association. (2013). <i>Global University Rankings and their impact. Report II</i>, European University Association.. Belgium. 
         </li>
         <li>Fernández Tuesta, E., Garcia-Zorita, C., Rosario, R., &amp; Sanz-Casado, E. (2019). Does a Country/Region’s Economic Status Affect
            Its Universities’ Presence in International Rankings? <i>Journal of Data and Information Science</i>, <i>4</i>(2), 56–78. <a href="https://doi.org/10.2478/jdis-2019-0009">https://doi.org/10.2478/jdis-2019-0009</a>. 
         </li>
         <li>Fowles, J., Frederickson, H. G., &amp; Koppell, J. G. S. (2016a). University Rankings: Evidence and a Conceptual Framework.
            <i>Public Administration Review</i>, <i>76</i>(5), 790–803. <a href="https://dx.doi.org/10.1111/puar.12610">https://dx.doi.org/10.1111/puar.12610</a>. 
         </li>
         <li>Fowles, J., Frederickson, H. G., &amp; Koppell, J. G. S. (2016b). University Rankings: Evidence and a Conceptual Framework.
            <i>Public Administration Review</i>, <i>76</i>(5), 790–803. <a href="https://dx.doi.org/10.1111/puar.12610">https://dx.doi.org/10.1111/puar.12610</a>. 
         </li>
         <li>Ganga-Contreras, F., San Martin, W. S. S., &amp; Viancos, P. (2019). University rankings as a tool for institutional evaluation: an
            analysis of used methodologies in international instruments. <i>Revista inclusiones</i>, <i>6</i>(4), 367–382. 
         </li>
         <li>García-Aracil, A., Gabaldón, D., Mora, J.-G., &amp; Vila, L. E. (2007). The relationship between life goals and fields of study
            among young European graduates. <i>Higher Education</i>, <i>53</i>, 843–865. <a href="https://dx.doi.org/10.1007/s10734-005-7517-3">https://dx.doi.org/10.1007/s10734-005-7517-3</a>. 
         </li>
         <li>Goglio, V. (2016). One size fits all? A different perspective on university rankings. <i>Journal of Higher Education Policy and Management</i>, <i>38</i>(2), 212–226. <a href="https://doi.org/10.1080/1360080x.2016.1150553">https://doi.org/10.1080/1360080x.2016.1150553</a>. 
         </li>
         <li>Gupta, M., Shaheen, M., &amp; Reddy, K. P. (2018). <i>Qualitative techniques for workplace data analysis</i>, IGI Global. United States. <a href="https://doi.org/10.1111/ejed.12064">https://doi.org/10.1111/ejed.12064</a>. 
         </li>
         <li>Hazelkorn, E. (2008). Learning to Live with League Tables and Ranking: The Experience of Institutional Leaders. <i>Higher Education Policy</i>, <i>21</i>, 193–215. <a href="https://dx.doi.org/10.1057/hep.2008.1">https://dx.doi.org/10.1057/hep.2008.1</a>. 
         </li>
         <li>Hazelkorn, E., Loukkola, T., &amp; Zhang, T. (2013). <i>Rankings in Institutional Strategies and Processes: Impact or Illusion?</i>, European University Association. Belgium. Retrieved from <a href="https://eua.eu/downloads/publications/rankings%20in%20institutional%20strategies%20and%20processes%20impact%20or%20illusion.pdf">https://eua.eu/downloads/publications/rankings%20in%20institutional%20strategies%20and%20processes%20impact%20or%20illusion.pdf</a>. 
         </li>
         <li>Horstschräer, J. (2012). University rankings in action? The importance of rankings and an excellence competition for university
            choice of high-ability students. <i>Economics of Education Review</i>, <i>31</i>(6), 1162–1176. <a href="https://doi.org/10.1016/j.econedurev.2012.07.018">https://doi.org/10.1016/j.econedurev.2012.07.018</a>. 
         </li>
         <li>Jabnoun, N. (2015). The influence of wealth, transparency, and democracy on the number of top ranked universities. <i>Quality Assurance in Education</i>, <i>23</i>(2), 108–122. <a href="https://dx.doi.org/10.1007/s11192-012-0749-y">https://dx.doi.org/10.1007/s11192-012-0749-y</a>. 
         </li>
         <li>Kaycheng, S. (2015). What the Overall doesn’t tell about world university rankings: examples from ARWU, QSWUR, and THEWUR
            in 2013. <i>Journal of Higher Education Policy and Management</i>, <i>37</i>(3), 295–307. <a href="https://doi.org/10.1080/1360080X.2015.1035523">https://doi.org/10.1080/1360080X.2015.1035523</a>. 
         </li>
         <li>Kehm, B.M. (2014). Global University Rankings - Impacts and Unintended Side Effects. <i>European Journal of Education</i>, <i>49</i>(1), 102–112. <a href="https://dx.doi.org/10.1111/ejed.12064">https://dx.doi.org/10.1111/ejed.12064</a>. 
         </li>
         <li>Lauder, A., Sari, R., Suwartha, N., &amp; Tjahjono, G. (2015). Critical review of a global campus sustainability ranking: GreenMetric.
            <i>Journal of Cleaner Production</i>, <i>108</i>, 852–863. <a href="https://doi.org/10.1016/j.jclepro.2015.02.080">https://doi.org/10.1016/j.jclepro.2015.02.080</a>. 
         </li>
         <li>Lim, M. (2017). The building of weak expertise: the work of global university rankers. <i>Higher Education</i>, <i>75</i>, 415–430. <a href="https://doi.org/10.1007/s10734-017-0147-8">https://doi.org/10.1007/s10734-017-0147-8</a>. 
         </li>
         <li>Locke, W. (2014). The intensification of rankings logic in an increasingly marketised higher education environment. <i>European Journal of Education</i>, <i>49</i>(1), 77–90. <a href="https://doi.org/10.1111/ejed.12060">https://doi.org/10.1111/ejed.12060</a>. 
         </li>
         <li>Lynch, K. (2015). Control by numbers: new managerialism and ranking in higher education. <i>Critical Studies in Education</i>, <i>56</i>(2), 190–207. <a href="https://doi.org/10.1080/17508487.2014.949811">https://doi.org/10.1080/17508487.2014.949811</a>. 
         </li>
         <li>Margison, S. (2014). University Rankings and Social Science. <i>European Journal of Education</i>, <i>49</i>(1), 45–59. <a href="https://doi.org/10.1111/ejed.12061">https://doi.org/10.1111/ejed.12061</a>. 
         </li>
         <li>Mengual-Andrés, S. (2013). Rethinking the role of Higher Education. <i>Journal of New Approaches in Educational Research</i>, <i>2</i>(1), 01–02. <a href="https://doi.org/10.7821/naer.2.1.1-2">https://doi.org/10.7821/naer.2.1.1-2</a>. 
         </li>
         <li>Millot, B. (2015). International rankings: Universities vs. higher education systems. <i>International Journal of Educational Development</i>, <i>40</i>, 156–165. <a href="https://doi.org/10.1016/j.ijedudev.2014.10.004">https://doi.org/10.1016/j.ijedudev.2014.10.004</a>. 
         </li>
         <li>Mora, J.-G., Ferreira, C., Vidal, M.-J., &amp; Vieira, M. (2015). Higher education in Albania: developing third mission activities.
            <i>Tertiary Education and Management</i>, <i>21</i>, 29–40. <a href="https://dx.doi.org/10.1080/13583883.2014.994556">https://dx.doi.org/10.1080/13583883.2014.994556</a>. 
         </li>
         <li>O'Connell, C. (2015). An examination of global university rankings as a new mechanism influencing mission differentiation:
            the UK context. Tertiary Education and Management. <i>Tertiary Education and Management</i>, <i>21</i>, 111–126. <a href="https://doi.org/10.1080/13583883.2015.1017832">https://doi.org/10.1080/13583883.2015.1017832</a>. 
         </li>
         <li>Olcay, G., &amp; Bulu, M. (2017). Is measuring the knowledge creation of universities possible? A review of university rankings.
            <i>Technological Forecasting Social Change</i>, <i>123</i>, 153–160. <a href="http://doi.org/10.1016/j.techfore.2016.03.029">http://doi.org/10.1016/j.techfore.2016.03.029</a>. 
         </li>
         <li>Pavel, A. (2015). Global university rankings - a comparative analysis. <i>Procedia Economics and Finance</i>, <i>26</i>, 54–63. <a href="https://doi.org/10.1016/S2212-5671(15)00838-2">https://doi.org/10.1016/S2212-5671(15)00838-2</a>. 
         </li>
         <li>Pérez-Díaz, V., &amp; Rodríguez, J. (2014). <i>La comunidad universitaria Española opine [The opinion of the Spanish University Community</i>. Madrid:  Fundación Europea Sociedad y Educación. 
         </li>
         <li>Perez Mejias, P., Chiappa, R., &amp; Guzmán-Valenzuela, C. (2018). Privileging the Privileged: The Effects of International University Rankings
            on a Chilean Fellowship Program for Graduate Studies Abroad. <i>Social Sciences</i>, <i>7</i>(243), 2–23. <a href="https://doi.org/10.3390/socsci7120243">https://doi.org/10.3390/socsci7120243</a>. 
         </li>
         <li>Tofallis, C. (2012). A different approach to university rankings. <i>Higher Education</i>, <i>63</i>, 1–18. <a href="https://doi.org/10.1007/s10734-011-9417-z">https://doi.org/10.1007/s10734-011-9417-z</a>. 
         </li>
         <li>Vladimirovich, N., &amp; Nikolayevna, I. (2013). University rankings as a tool to enhance competitiveness, clustering and transnational
            governance of higher education in the context of globalization. <i>Middle-East Journal of Scientific Research</i>, <i>16</i>(3), 357–361. 
         </li>
         <li>Wilson, L. (2011). Global University Rankings and their impact. EUA Rankings Review. Paper presented at SEFI Conference. Lisbon.
         </li>
      </ol>
   </article>
</body>
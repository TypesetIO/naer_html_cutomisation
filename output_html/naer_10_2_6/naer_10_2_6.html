<body>
   <article>
      <h1>INTRODUCTION</h1>
      <p>The process of skill acquisition by university students requires an optimization of the relationship between practice and
         theory in academic and professional contexts, using multimedia resources and in online environments. These should mirror the
         professional reality as closely as possible even in situations where virtual audio-visual strategies need to be used. Understanding
         how to deliver this practice means analysing electronic tasks  (<a href="#" id="bib18">Kandari, Qattan, &amp; M, 2020</a>). This can be done through multimedia annotation that helps interpret the message.
      </p>
      <p>It is necessary to focus on both methodologies and technologies that allow the analysis of these multimedia messages. Together,
         they have more of an impact  (<a href="#" id="bib1">Becker, 2010</a>). An annotation is a portion of information that is associated with a piece of original content in order to explain something
         about that content or to add more information  (<a href="#" id="bib13">Gayoso-Cabada, Sarasa-Cabezuelo, &amp; Sierra-Rodríguez, 2019</a>). In academic settings, annotations can be used by students and teachers to tag and highlight texts, images, songs, websites,
         videos and other resources  (<a href="#" id="bib30">Novak, Razzouk, &amp; Johnson, 2012</a>; <a href="#" id="bib37">Sauli, Cattaneo, &amp; Meij, 2018</a>; <a href="#" id="bib38">Smith, Blankinship, &amp; Lackner, 2000</a>; <a href="#" id="bib41">Zhu, Chen, Avadhanam, Shui, &amp; Zhang, 2020</a>). This study uses multimedia annotations (hereafter MA) to select and break down these messages to provide reasonings in
         the shape of commentaries and interpretations as well as sharing them through social tags. This is defined as "the collective
         action of users associating tags to resources they have created and experimented with"  (<a href="#" id="bib19">Lau, Lee, &amp; Singh, 2015</a>).
      </p>
      <p>Although their origin dates back to papyrus annotations in ancient Greece  (<a href="#" id="bib29">Muellner, 2015</a>) they have experienced a boom with the development of MA software, especially image and video annotation software, with promising
         results in terms of user usability and effectiveness in video analysis  (<a href="#" id="bib2">Bianco, Ciocca, Napoletano, &amp; Schettini, 2015</a>; <a href="#" id="bib6">Chen, Chen, Xu, March, &amp; Benford, 2007</a>; <a href="#" id="bib28">Monedero, Cebrián, &amp; Desenne, 2015</a>). 
      </p>
      <p>Many studies on shared MAs have been carried out in the academic field  (<a href="#" id="bib7">Colasante, 2011</a>; <a href="#" id="bib10">Dias-Pereira-Dos-Santos, Loke, &amp; Martinez-Maldonado, 2018</a>; <a href="#" id="bib25">McFadden, Ellis, Anwar, &amp; Roehrig, 2014</a>; <a href="#" id="bib30">Novak et al., 2012</a>; <a href="#" id="bib33">Paradis &amp; Fendt, 2016</a>; <a href="#" id="bib34">Pérez-Torregrosa, Díaz-Martín, &amp; Ibáñez-Cubillas, 2017</a>; <a href="#" id="bib37">Sauli et al., 2018</a>; <a href="#" id="bib39">Su, Yang, Hwang, &amp; Zhang, 2010</a>; <a href="#" id="bib40">Sydnor, 2016</a>). At present, before and during the COVID-19 pandemic, they have been used successfully for the development of tasks in online
         teaching programs (<a href="#" id="bib41">Zhu et al., 2020</a>).
      </p>
      <p>The research focuses on the analysis of the relationship between theory and practice through students' reasoning and argumentation
         by reading, viewing, reflecting and writing commentaries in MAs. To begin with, the argumentations were analysed in order
         to develop reflective practices in academic and professional settings  (<a href="#" id="bib4">Cebrián, Pérez, &amp; Cebrián, 2017</a>; <a href="#" id="bib23">Liu &amp; Stapleton, 2014</a>; <a href="#" id="bib31">Nussbaum, Sinatra, &amp; Poliquin, 2008</a>). Evidence shows that "scaffolding computer-mediated discussions can improve the quality of argumentation in students' writing"
         (<a href="#" id="bib32">Özçinar, 2015</a>).
      </p>
      <p>Students' thinking must be constructed from the analysis of professional good practices, that is why their presentation methods
         are an object of research. This is especially the case in written texts and/or videos  (<a href="#" id="bib8">Debbag &amp; Fidan, 2020</a>; <a href="#" id="bib15">Hefter &amp; Berthold, 2020</a>; <a href="#" id="bib20">Lee &amp; List, 2019</a>). Texts allow for a more relaxed reading of the message which becomes more contextualized when texts and images are combined
         to produce argumentation and exchange of meanings  (<a href="#" id="bib38">Smith et al., 2000</a>). At the same time, the speed of the video can be changed to emphasize the emotional parts and the maker's guided narrative
         (<a href="#" id="bib17">Imran, Cheikh, &amp; Kowalski, 2016</a>).
      </p>
      <p>Various studies have proposed collaborative annotation of learning resources; however, little research has been carried out
         on the classification mechanisms used in the annotation tools. The research by  (<a href="#" id="bib13">Gayoso-Cabada et al., 2019</a>) identifies four mechanisms: classifications based on controlled vocabulary, folksonomies, ontologies and absence of classification
         mechanisms. Annotation classification plays an essential role in the application of MA in education, but the main problem
         with folksonomies is the open nature of these terms. Collaborative tagging systems, also known as folksonomy, have gained
         popularity as they easily organize resource content (web pages, images and videos, among others) using open tags. Users can
         therefore provide information and create a rich and growing corpus of social knowledge that can be used by recommendation
         technologies  (<a href="#" id="bib14">Godoy &amp; Corbellini, 2016</a>). In this context, the folksonomy's three-way relationship between users, resources and tags presents new challenges, the
         goal being to help users by means of recommendation systems  (<a href="#" id="bib16">Hsu, 2013</a>; <a href="#" id="bib19">Lau et al., 2015</a>). 
      </p>
      <p>As folksonomy research is still quite new, the theoretical perspective and research methods are still being developed. In
         the university environment multimedia resources are used to illustrate the process, with videos being widely employed and
         preferred by students; however, the strategies that students use to understand its contents are not known in depth  (<a href="#" id="bib21">List &amp; Ballenger, 2019</a>; <a href="#" id="bib22">List, 2018</a>). Moreover, it is also important to find out which is the best design to encourage the critical and reflective learning of
         these realities and to better understand how the design of these methodologies and tasks that are taught with multimedia resources
         influence the quality of student annotations.
      </p>
      <p>The purpose of this study is to understand how the type of format chosen and the instructions given in the tuition exercises
         and tasks influence the quality of the students' annotations. The idea is also to implement other innovative educational processes
         through task assignments: guiding students through the annotation process, explaining to them how the process is to be carried
         out and giving teachers information on how students write and reason about the practical and innovative content. Hence the
         need to investigate the effect of these MA mechanisms. 
      </p>
      <p>This study is part of a broader line of research,<sup>1</sup><sup>2</sup> that, in this paper, will focus on the analysis of the quality of responses generated by students when they use MAs to comment
         and reason about an innovative educational project, based on the format or code (text vs. video) and the instructions that
         are supplied in the task (make narrow annotations –with tags given by the teacher– vs. broad annotations –with no given tags).
         The objectives of this research are the following: 1. Analysing the levels of quality and quantity of the responses generated
         by students when they make annotations to define and justify an innovative educational project. 2. Studying to what extent
         the pre-set tags given by the teacher to guide the task and the format used (text vs. video) affect the variation of the MAs
         generated by the students.
      </p>
      <h1>MATERIAL AND METHODS</h1>
      <h2>Participants</h2>
      <p>274 Educational Science undergraduate students (mean age 20 years) took part in this study. They are studying Educational
         Technology in Spain and know and understand the purpose of the research, which has been suggested by the teachers (among several
         others on offer), with which they agree and that does not have a direct impact on their grades. The information was collected
         over two academic years.
      </p>
      <h2>Design</h2>
      <p>The study is mixed research –quantitative and qualitative– and the instrument is in the tool itself Coannotation.com which
         collects all the steps and data generated by users in the proposed activities. Being these data collected and analysed both
         qualitative and quantitative natura, as shown in Figure  1, above right, where we can also see the button to export the data to excel format for analysis. The replication of this study
         can be done with the same tool and activity description proposed below. The MAs written by the students were analysed quantitatively
         and qualitatively through shared messages in an innovative educational project entitled <i>Class of Clans</i>. The project was selected by Fundación Telefónica from the 100 best in the country. Focusing on gamification, it has won
         awards from SIMO Education and the Spanish Ministry of Education (MEC) for educational innovation and for teaching teams.
         It is based on the integration of four subjects from the first year of secondary education: Natural Sciences, Social Sciences,
         Technology and Arts. 
      </p>
      <p>The MAs were divided into groups, according to the independent variables of the study: on the one hand, <i>folksonomy</i> (broad vs. narrow) and on the other hand, the <i>message code</i> (video vs. text). In the first group the MAs were analysed according to the task's instructions, i.e. making a free annotation
         (<i>broad folksonomy</i>) with tags created by the students, or to provide them with tags (<i>narrow folksonomy</i>) in order to produce said annotations. In the second, i.e. the <i>message code</i>, the content of the messages was prepared for dissemination by the authors of the project. The text message contains structured
         information and complies with the categories of the educational innovation project (Fundación Telefónica, <a href="#" id="bib11">2016</a>, 68-70). The video message does not use voiceover, but rather recorded images of student actions, group tasks, activities
         in the school garden... with a very emotive soundtrack and text labels to involve the viewer in the challenge of the game.
      </p>
      <h2>Procedure</h2>
      <p>The argumentation on the texts and videos created with the MA methodology through online groups discussions, was carried out
         using new tools such as Coannotation.com (developed in the [3] project), which creates a layer of annotations from a YouTube
         video. Annotations are exported to Excel to be studied. Google Drive and Annotation Studio (http://uma.annotationstudio.org/)
         were used for text annotations. The latter is a tool created by MIT’s Digital Humanities Laboratory' HyperStudio (EEUU) http://hyperstudio.mit.edu
         (Paradis &amp; Fendt, 2016) and used in the Vidanet project to test the educational institutions participating in this study.
      </p>
      <p>The instructions given to the subgroups vary according to the <i>folksonomy</i>, not to the two codes (text and video). For <i>the broad folksonomy </i> the instructions are: “<i>Read and analyze the text and/or watch the video of the Class of Clans project. Next, make four annotations of the most important
            aspects that the message triggers in you and the reasons why you consider it to be an innovative project, reasoning and justifying
            in each annotation why it is innovative</i>”. For the <i>narrow folksonomy</i> they are: “<i>Read and analyze the text and/or watch the video of the Class of Clans project. Next, make four annotations of the most important
            aspects that the message triggers in you and the reasons why you consider it to be an innovative project, discussing and justifying
            why it is innovative in each annotation</i>". <i>Always use these four tags: <b>Problem</b>: What problem does innovation try to solve? <b>Solution</b>: What solution does innovation suggest? <b>Evidence</b>: what type of impact does ICT have? <b>Competence</b>: What skills does this innovation foster?”</i></p>
      <p>The study sample was chosen for convenience with 274 students from different degrees (Pedagogy and Primary Degree) from the
         Faculties of Education of the University of Granada and Malaga for two academic years 2016-17 and 2017-18. The students were
         distributed into subgroups according to the instructions and codes, obtaining a total of 845 MAs (Table  1). However, in the text annotations task fewer annotations were collected because of a logistical problem unrelated to the
         research.
      </p>
      <table>
         <caption><strong>Table 1. </strong>Annotations according to the two research variables
         </caption>
         <thead>
            <tr>
               <th>
                  <p>No. of students</p>
               </th>
               <th colspan="2">
                  <p>Broad annotations</p>
               </th>
               <th colspan="2">
                  <p>Narrow annotations</p>
               </th>
               <th>
                  <p>Total video</p>
               </th>
               <th>
                  <p>Total text</p>
               </th>
               <th>
                  <p>Total annotations</p>
               </th>
            </tr>
            <tr>
               <th></th>
               <th>
                  <p>video</p>
               </th>
               <th>
                  <p>text</p>
               </th>
               <th>
                  <p>video</p>
               </th>
               <th></th>
               <th></th>
               <th></th>
               <th></th>
            </tr>
         </thead>
         <tbody>
            <tr>
               <td>
                  <p>274</p>
               </td>
               <td>
                  <p>243</p>
               </td>
               <td>
                  <p>74</p>
               </td>
               <td>
                  <p>461</p>
               </td>
               <td>
                  <p>67</p>
               </td>
               <td>
                  <p>704</p>
               </td>
               <td>
                  <p>141</p>
               </td>
               <td>
                  <p>845</p>
               </td>
            </tr>
         </tbody>
      </table>
      <h2>Data Analysis</h2>
      <p>After the students created the MAs, the researchers exported the annotations to Excel and then to SPSS for quantitative analysis.
         Qualitative and quantitative analysis are combined. The treatment of the data implies two strategies of analysis: the first
         one using Coannotation.com's statistical graph where the annotations (yellow lines) are grouped into "peaks" and "valleys"
         (Figure  1).
      </p>
      <figure><img src="f509edfa-d9e2-443d-a2f9-aa591236103e-u664-3714-2-sp.jpg" alt=""><figcaption><strong>Figure 1. </strong>Video interface in Coannotation with "peaks" and "valleys" representing the number of annotations generated
         </figcaption>
      </figure>
      <p>This quantitative analysis helped focus the attention on certain areas, for example, whether there are more or fewer annotations
         on the video timeline (which we named "peaks" and "valleys" respectively), as well as the result of the MA quality rating
         scale. In the second approach, all the annotations, both narrow and broad, are analyzed according to the four initial tags,
         categorizing the broad ones and adding two more extracted from the data (<i><b>Good Practices</b></i><i> –the innovative educational project represents a good practice that can be applied in other situations– and <b>Others</b> –different comments not included in any tag</i>). This qualitative analysis of MAs is carried out through content analysis by categories, with the idea of performing a "Q-analysis"
         or "Connectivity Analysis"  (<a href="#" id="bib3">Buendía, Colás, &amp; Hernández, 1998</a>) . 
      </p>
      <p>The quality of the responses is also evaluated with a rating scale of 0 to 3, extracted from an argumentative rubric  (<a href="#" id="bib4">Cebrián et al., 2017</a>). Value 0 means that the answer has no relation to the tag or question posed in the task in general. Value 1 applies to MAs
         that contain a mere statement and/or assessment without justification. Value 2 is applied when the student tries to explain
         and/or justify and Value 3 when there is a broad explanation, reasoned and justified, that denotes a deeper reading and analysis
         of the message. These levels were not known by the students either before or after the task; what was basically taken into
         account in their assessment was the coherence with the tag rather than other comments and/or reasoning.
      </p>
      <h1>RESULTS </h1>
      <h2>Tags in the MAs</h2>
      <p>First, we analysed the quality and quantity of responses of the students' annotations on Coannotation.com. The resulting percentage
         of tag appearance is: Competence (20%), Evidence (16,5%), Problem (16,5%) and Solution (14%). The remaining 33% is highly
         divided into other headings such as "critical capacity", "cooperation", "didactics", "gamification", "methodology", "teachers",
         "active" and "reflection".
      </p>
      <p>Figure  2  shows a similar trend in the frequency distribution of the tags, regardless of the folksonomy, although the total is higher
         in <i>narrow folksonomy</i> (528). Among the most frequent labels we find "solution" (84) and "competence" (79) in the case of <i>broad folksonomy</i>, the latter being slightly higher in <i>narrow folksonomy </i> (155). 
      </p>
      <figure><img src="e9fe413c-6b39-4cee-8fe1-3d2c689de4c1-ufigure-2-u-left_frequency-of-tags-according-to-both-codes-in-broad-folksonomy_right-_same-in-narrow-folksonomy-u.png" alt=""><figcaption><strong>Figure 2. </strong> Left: Frequency of tags according to both codes in broad <i>folksonomy</i> Right: Same in <i>narrow folksonomy</i>.
         </figcaption>
      </figure>
      <p>There is a difference in the total number of responses according to the folksonomy, 317 in <i>broad folksonomy</i> and 528 in <i>narrow folksonomy</i>, this difference being greater in the video than in the text. Allegedly this occurs because when the task instructions are
         to apply the four tags established by the teacher, students are more committed than when they can supply four answers without
         prefixed tags (Figure  2). The possible reasons to explain why there are differences in the video but not in the text may be that when viewing the
         video, viewers are somewhat more "caught up" in the storyline, and their attention is possibly more dispersed and/or less
         concentrated. However, this is not the case when previous instructions are mentioned in the video, i.e. students were more
         "committed" to the task in the video when they had this instruction than when they did not. 
      </p>
      <p>As for the quality of the tags (according to the 0-3 rating), there are significant differences in the mean values of best
         and worst quality, respectively (tags "Solution" X= 1.45 and "Others" X=0.42), with no significantly different quality values
         in the rest (Table  2).
      </p>
      <table>
         <caption><strong>Table 2. </strong>Average and median differences according to tags
         </caption>
         <thead>
            <tr>
               <th>
                  <p>Tags</p>
               </th>
               <th>
                  <p>Mean</p>
               </th>
               <th>
                  <p>N</p>
               </th>
               <th>
                  <p>Standard Deviation</p>
               </th>
               <th>
                  <p>Median</p>
               </th>
            </tr>
         </thead>
         <tbody>
            <tr>
               <td>
                  <p>Problem</p>
               </td>
               <td>
                  <p>1.27</p>
               </td>
               <td>
                  <p>135</p>
               </td>
               <td>
                  <p>.973</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Solution</p>
               </td>
               <td>
                  <p>1.45</p>
               </td>
               <td>
                  <p>214</p>
               </td>
               <td>
                  <p>.766</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Evidence</p>
               </td>
               <td>
                  <p>1.10</p>
               </td>
               <td>
                  <p>156</p>
               </td>
               <td>
                  <p>.844</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Competence</p>
               </td>
               <td>
                  <p>1.22</p>
               </td>
               <td>
                  <p>234</p>
               </td>
               <td>
                  <p>.731</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Good practices</p>
               </td>
               <td>
                  <p>1.20</p>
               </td>
               <td>
                  <p>41</p>
               </td>
               <td>
                  <p>.679</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Others</p>
               </td>
               <td>
                  <p> .42</p>
               </td>
               <td>
                  <p>65</p>
               </td>
               <td>
                  <p>.583</p>
               </td>
               <td>
                  <p>.00</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Total</p>
               </td>
               <td>
                  <p>1.20</p>
               </td>
               <td>
                  <p>845</p>
               </td>
               <td>
                  <p>.831</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
         </tbody>
      </table>
      <p>The Kruskal-Wallis statistical test shows no differences in the tags "Problem", "Evidence", "Competence" and "Good Practice".
         On the other hand, there were differences in "Solution" and "Others". The tag "Solution" is the one with the highest frequency
         of annotations (Table  3) and the tag "Others" does not respond to any of the questions posed, as comments or opinions mostly score a 0 rating.
      </p>
      <table>
         <caption><strong>Table 3. </strong>Differences in the answers according to the quality of the tags "Solution" and "Other" as per the Kruskal-Wallis test
         </caption>
         <thead>
            <tr>
               <th>
                  <p>Type of tags</p>
               </th>
               <th>
                  <p>N</p>
               </th>
               <th>
                  <p>N</p>
               </th>
               <th>
                  <p>Average Range</p>
               </th>
               <th>
                  <p>Quality of Answer</p>
               </th>
            </tr>
         </thead>
         <tbody>
            <tr>
               <td>
                  <p>Problem</p>
               </td>
               <td>
                  <p>135</p>
               </td>
               <td>
                  <p>436.67</p>
               </td>
               <td>
                  <p>Chi square</p>
               </td>
               <td>
                  <p>85.696</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Solution</p>
               </td>
               <td>
                  <p>214</p>
               </td>
               <td>
                  <p>495.39</p>
               </td>
               <td>
                  <p>df</p>
               </td>
               <td>
                  <p>5</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Evidence</p>
               </td>
               <td>
                  <p>156</p>
               </td>
               <td>
                  <p>396.44</p>
               </td>
               <td>
                  <p>Asymptotic</p>
               </td>
               <td>
                  <p>.000</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Competence</p>
               </td>
               <td>
                  <p>234</p>
               </td>
               <td>
                  <p>428.29</p>
               </td>
               <td>
                  <p>sig.</p>
               </td>
               <td></td>
            </tr>
            <tr>
               <td>
                  <p>Good practices</p>
               </td>
               <td>
                  <p>41</p>
               </td>
               <td>
                  <p>421.12</p>
               </td>
               <td></td>
               <td></td>
            </tr>
            <tr>
               <td>
                  <p>Others</p>
               </td>
               <td>
                  <p>65</p>
               </td>
               <td>
                  <p>202.16</p>
               </td>
               <td></td>
               <td></td>
            </tr>
            <tr>
               <td>
                  <p>Total</p>
               </td>
               <td>
                  <p>566</p>
               </td>
               <td></td>
               <td></td>
               <td></td>
            </tr>
         </tbody>
      </table>
      <h2>Differences According to Folksonomy</h2>
      <p>The results do not show significant differences in the quality of responses according to the code in the case of <i>broad folksonomy</i>, but there are significant differences in <i>narrow folksonomy</i> (Table  4) regardless of the code used.
      </p>
      <table>
         <caption><strong>Table 4. </strong>Analysis according to broad vs. <i>narrow folksonomy</i></caption>
         <thead>
            <tr>
               <th>
                  <p>Folksonomy</p>
               </th>
               <th>
                  <p>Quality of the Answer</p>
               </th>
               <th></th>
            </tr>
         </thead>
         <tbody>
            <tr>
               <td>
                  <p>Broad</p>
               </td>
               <td>
                  <p>Mann-Whitney U</p>
               </td>
               <td>
                  <p>8.887.000</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Wilcoxon W</p>
               </td>
               <td>
                  <p>11.662.000</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Z</p>
               </td>
               <td>
                  <p>-.162</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>(Bilateral) asymptotic sig.</p>
               </td>
               <td>
                  <p>.872</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Narrow</p>
               </td>
               <td>
                  <p>Mann-Whitney U</p>
               </td>
               <td>
                  <p>12.128.000</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Wilcoxon W</p>
               </td>
               <td>
                  <p>118.619.000</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Z</p>
               </td>
               <td>
                  <p>-3.048</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>(Bilateral) asymptotic sig.</p>
               </td>
               <td>
                  <p>.002</p>
               </td>
            </tr>
         </tbody>
      </table>
      <p>In contrast, there are no significant differences in the quality of student responses when the <i>folksonomy</i> variable is used, regardless of the code, text or video (Table  5).
      </p>
      <table>
         <caption><strong>Table 5. </strong>Differences according to <i>folksonomy</i></caption>
         <thead>
            <tr>
               <th></th>
               <th>
                  <p>Quality of the Answer</p>
               </th>
            </tr>
         </thead>
         <tbody>
            <tr>
               <td>
                  <p>Mann-Whitney U</p>
               </td>
               <td>
                  <p>81.647.500</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Wilcoxon W</p>
               </td>
               <td>
                  <p>221303.50</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Z</p>
               </td>
               <td>
                  <p>-.637</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>(Bilateral) asymptotic sig.</p>
               </td>
               <td>
                  <p>.524</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Grouping variable: folksonomy</p>
               </td>
               <td></td>
            </tr>
         </tbody>
      </table>
      <p>There is not enough evidence to state that there are significant differences in terms of <i>folksonomy</i> in the video and in the text separately and together. However, the quality of the answers is greater overall when the <i>folksonomy</i> is narrow. As shown in Figure  3, the quality of MAs in the text is higher in narrow <i>folksonomy</i> (left graphic red line ATE) and the same happens in the video (right graphic red line AVE).
      </p>
      <figure><img src="17293874-c6da-4993-816f-6530b21c1b26-ufigure-3-u-left-mean-values-of-ma-quality-in-the-text-according-to-narrow-ate-vs-u-broad-ata-folksonomy-u-right-same-in-the-video-according-to-narrow-ave-vs-broad-ava-folksomomy.jpg" alt=""><figcaption><strong>Figure 3. </strong>Left: Mean values of MA quality in the text according to <i>narrow (ATE) vs. broad (ATA) folksonomy.</i> Right: Same in the video according to <i>narrow (AVE) vs broad (AVA) folksomomy</i></figcaption>
      </figure>
      <h2>Differences According to the Code</h2>
      <p>The results show that there are significant differences in the quality of the answers depending on the code used (text vs.
         video), the quality of the annotations in the text being higher than in the video, regardless of the <i>folksonomy</i>. In both cases the median is the same, i.e. 1 (Table  6). This may be because the text message is more structured and responds to the issues raised in an educational innovation
         project (tags and questions provided by teachers). While some students had the textual information that they could potentially
         use when analysing the video, this did not happen in the majority of the cases, Therefore, it cannot be said that prior text
         instructions had an influence on the result. 
      </p>
      <p>In the case of <i>narrow folksonomy</i>, the use of text obtains answers with a better rating, with a median in both cases of 1 (Table  6).
      </p>
      <table>
         <caption><strong>Table 6. </strong>Analysis formats <i>text vs videos</i> according to <i>folksonomy</i></caption>
         <thead>
            <tr>
               <th>
                  <p>Folksonomy</p>
               </th>
               <th>
                  <p>Format</p>
               </th>
               <th>
                  <p>Mean</p>
               </th>
               <th>
                  <p>N</p>
               </th>
               <th>
                  <p>Standard deviation</p>
               </th>
               <th>
                  <p>Median</p>
               </th>
            </tr>
         </thead>
         <tbody>
            <tr>
               <td>
                  <p>Broad</p>
               </td>
               <td>
                  <p>Text</p>
               </td>
               <td>
                  <p>1.23</p>
               </td>
               <td>
                  <p>74</p>
               </td>
               <td>
                  <p>.930</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Video</p>
               </td>
               <td>
                  <p>1.23</p>
               </td>
               <td>
                  <p>243</p>
               </td>
               <td>
                  <p>.816</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Total</p>
               </td>
               <td>
                  <p>1.23</p>
               </td>
               <td>
                  <p>317</p>
               </td>
               <td>
                  <p>.842</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td>
                  <p>Narrow</p>
               </td>
               <td>
                  <p>Text</p>
               </td>
               <td>
                  <p>1.43</p>
               </td>
               <td>
                  <p>67</p>
               </td>
               <td>
                  <p>.701</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Video</p>
               </td>
               <td>
                  <p>1.15</p>
               </td>
               <td>
                  <p>461</p>
               </td>
               <td>
                  <p>.835</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
            <tr>
               <td></td>
               <td>
                  <p>Total</p>
               </td>
               <td>
                  <p>1.19</p>
               </td>
               <td>
                  <p>528</p>
               </td>
               <td>
                  <p>.824</p>
               </td>
               <td>
                  <p>1.00</p>
               </td>
            </tr>
         </tbody>
      </table>
      <h2>Quality and Quantity of the MAs</h2>
      <p>In general, it can be seen the commitment when creating annotations is with the teacher or the task, but not with learning;
         that is to say, the quality of the answers given as a whole and by separate formats is different mainly due to the fact that
         the text code is more structured and has a prefixed tag. It can therefore be stated that narrow folksonomy and the text commit
         and focus the analysis in terms of number and quality of responses.
      </p>
      <p>According to the results obtained, there is a greater quality response in text annotations in both folksonomies, which confirms
         the previous result that videos "use up" more concentration and it is more difficult to maintain an own and parallel thought,
         which is therefore more "dispersed". Thus, once the students have watched the video, they make their annotations in those
         images or sequences that produced an impact, and that were altogether fewer and of lower quality than those required by narrow
         folksonomy. 
      </p>
      <p>Likewise, and reinforcing the above, it is interesting to observe that the text annotations made by the students without given
         tags contain more words and more explanation as they are, in some cases, backed up with quotations and references to reinforce
         their reasoning. The total number of words in the four subgroups is: AVA 7828 AVE 11577 ATA 2540 ATE 9819. The chi-square
         indicates that there is also a 99% probability that there is an association between the variables.
      </p>
      <p>The MAs show that the type of response with the tag "Solution" responds fundamentally to the purpose that the students thought
         the innovative educational project had as a whole, i.e. an effective, appropriate and original solution for the use of technologies,
         with comments such as <i>"it is innovative because it</i><i>breaks with the structure and spaces of the traditional classroom</i> ” (#1281) Others found it innovative from the curricular point of view: “<i>This is a multidisciplinary project, since it includes four subjects of the curriculum</i>” (#1293).
      </p>
      <p>On the other hand, the annotations on Problems are surprising because of the low number of answers (only 6 in <i>wide folksonomy</i>), as if the students were less concerned about the problems of using technologies in the implementation of an educational
         innovation project. The issue of the reasons for innovation and technology only seldom appears in the students' notes: “<i>It is often very difficult to work with so many students at once and to get everyone's attention and get them involved. That
            is why I think this project is innovative, as making 51 students find it attractive and eager to get involved, is rather difficult</i>” (#1314).
      </p>
      <p>As for the quality of the answers rated from 0 to 3, the level does not exceed the average in all the tags. Few values reach
         the highest rating of 3, the reason being that they usually describe the fact without interpreting it, they do not explain
         what skill is being developed or why it is innovative, for instance: “<i>In this image we can see how the children are creating an urban garden. That is, in addition to learning mathematical knowledge,
            language, etc. They are also learning through environmental pedagogy</i>” (#6556). In other instances, the descriptions are general: “<i>It is a very innovative method, as it is a group research project where all the students of an ESO (secondary school) course
            participate, and it is all done through an online game</i>” (#6480). In a way, they explain the impact of ICT only by describing how the participants use it in the project, but very
         few MAs provide details of the skills that can be developed or the possibilities they actually have.
      </p>
      <h1>DISCUSSION AND CONCLUSIONS</h1>
      <p>The analysis of the students' MAs when they reason collaboratively on an innovative educational project allows discovery of
         if the type of mode (text vs video) and folksonomy (broad vs narrow) influences the quality of their responses. It has been
         demonstrated that there are significant differences in the quality and quantity of the responses, with the quality of the
         annotations in texts being somewhat higher, regardless of the <i>folksonomy</i>. These differences are mainly due to the use of texts and of <i>narrow folksonomy</i> (tags that the annotations need to follow), which produces a greater number and better quality of responses. Similar results
         have been found in other studies where texts required less time and effort than videos  (<a href="#" id="bib15">Hefter et al., 2020</a>). There are also other studies that point to advantages of texts over videos  (<a href="#" id="bib20">Lee et al., 2019</a>; <a href="#" id="bib21">List et al., 2019</a>). However, generating and creating video messages is more motivating than using texts for tasks such as the creation of diaries
         by the students  (<a href="#" id="bib8">Debbag et al., 2020</a>). Therefore, further research is needed on the functionalities of each code when using MAs.
      </p>
      <p>The differences found in this study may be due, firstly, to the fact that the text code contains a more structured presentation
         of the elements of an innovation project, so the answer can be sought according to the tags, with a greater number of words
         and reasoning. Second, narrow folksonomy means that students are more likely to respond according to the tags supplied. A
         similar process as to when a question cannot be left answered, students fill in all the tags because they are there to be
         filled. Thirdly, the video message is more exciting and evocative about the program, focusing on the innovative solution and
         the narrative of the video itself, factors that lead to a greater dispersion and subjectivity. This is why the tag "Solution"
         receives more answers, followed by "Competence". In short, and in line with the idea analysed by  (<a href="#" id="bib1">Becker, 2010</a>) on the combination of methodology and technology, in folksonomy, the audio-visual narrative of the video message, which
         is more open and subjective, is combined in folksonomy with an open assignment and design. The more structured text format
         with closed task assignment guides (narrow folksonomy) yields a higher number of annotations. Possibly, for different, more
         creative skills, broad folksonomy and video annotations might be more beneficial. In any case, this study has found that students
         have a greater, more focused and motivated commitment to the task when they share their interpretations, annotations and tags
         with others, similar to the findings by  (<a href="#" id="bib36">Qarabash, Heslop, Kharrufa, Balaam, &amp; Devlin, 2019</a>). Also, collaborative tagging activities with controlled vocabulary in the annotation are effective for the personalization
         of learning resources (<a href="#" id="bib19">Lau et al., 2015</a>) . However, although important steps have been taken in technological development and customization of learning  (<a href="#" id="bib16">Hsu, 2013</a>), more software development research is still needed to provide more personalized teaching and a closer relationship between
         user folksonomy and technological tagging.
      </p>
      <p>Students show a willingness to actively collaborate and get involved in tasks requiring reflection about the video's messages
         by analysing and writing shared annotations about the videos and texts. As in other research, the attention of students in
         the analysis of the messages increases by counteracting the speed of the message and its discourse  (<a href="#" id="bib35">Po-Sheng et al., 2018</a>), thus facilitating a more reflective and reasoned reading of the practical and innovative contents. Nonetheless, two important
         aspects need to be highlighted: In terms of methodology, teachers should follow up on the task, because at the beginning,
         as has happened in other studies  (<a href="#" id="bib25">McFadden et al., 2014</a>), students often do not make comments or ask questions to other students and only focus on their own annotations, which are
         lower-level reflections (descriptions and explanations) but not in-depth analysis. As for technology, in some cases and as
         in other studies  (<a href="#" id="bib12">Gao, 2013</a>), it is important to provide students with a simple and intuitive interface so they can manage the huge amount of annotations
         that are generated in the tasks, as well as having data visualization tools (statistical graphs, word clouds...) that allow
         an overview or a selection of a specific topic within this set of annotations (selective search tool for words or tags). This
         will be an important factor in the selection of tools in the future, hence, it is worthwhile proposing new comparative studies
         of the possibilities currently offered by MA systems from a technological point of view, together with new experiments and
         evaluations of their functionalities (Big data, AI, Machine Learning...), as well as review studies on the different methodologies
         and research results of the studies in diverse and INCLUSIVE contexts.
      </p>
      <p>The text is more structured and offers more information about impact and innovation, while the video only presents images
         of student actions, without voiceover or text tags that reinforce the images. Therefore, we cannot generalize the results
         to other contexts, and investigations on more exciting and less structured texts vs. more structured videos and more explanatory
         video messages should be carried out. More specifically, it would be interesting to add to the video a soundtrack with the
         same wording as the text and minimize the text exactly to the size of the video transcript. This would mean two messages with
         the same text with an audio-visual aid. In any case, more research should be done on how both codes can complement each other
         in online programs. This is in line the work by García-Martínez, Rigo-Carratalá &amp; Jiménez who seek to improve the reading
         process with multimedia resources. The results show minimal differences in the final evaluation and the authors state "...a
         Multimedia methodology to develop the textual comprehension, properly designed, helps the educational process to be more effective
         because students make fewer mistakes when answering questions. Furthermore, we have seen that at the time of the evaluation,
         the introduction of a multimedia methodology compared to a textual presentation does not improve (at least in absolute terms),
         the student’s ability to succeed in their responses." (<a href="#" id="bib24">2017</a>, p. 9). One option is a <i>combined methodological design</i> of the folksonomy variables and multimedia codes depending to the relevance, similar to the study by  (<a href="#" id="bib9">Dennen, Bagdy, &amp; Cates, 2018</a>) which examines student tagging activity within a five-week social bookmarking unit, considering that " these skills [tagging]
         are important components of information literacy and are used increasingly in professional settings where large quantities
         of information are being amassed, evaluated, and shared" (<a href="#" id="bib9">2018</a>, p. 117). 
      </p>
      <p>In future studies it will be necessary to experiment with the use of methodological design in different tasks and skills.
         It would also be advisable to study the relationship with internal vs. external motivation variables (e.g. final grades) and
         their use in broader contexts such as a complete program and different grades  (<a href="#" id="bib26">Mirriahi, Joksimović, Gašević, &amp; Dawson, 2018</a>; <a href="#" id="bib27">Mirriahi, Liaqat, Dawson, &amp; Gašević, 2016</a>). The very nature of the competence acquisition process demands it, because there is a profusion of technological innovations
         and multimedia messages in our social, family and professional life, and students need to consider these messages through
         a more critical and professional vision. It essential that university training is backed by documentation and multimedia resources
         of professional practices, and to create methodologies and activities that allow a greater theoretical-practical relationship
         with the aim of obtaining a deeper, more critical and reflective effort in the students' responses. In the current situation,
         greater flexibility is also needed in the university system in its teaching methods, adopting the methodologies and technologies
         used in the university environment by configuring an entire multimedia PLE-portfolio (<a href="#" id="bib5">Cebrián, 2017</a>).
      </p>
      <h1>ACKNOWLEDGEMENTS</h1>
      <p>Funded by: Ministry of Science and Innovation, Spain</p>
      <p>Funder Identifier: http://dx.doi.org/10.13039/501100004837</p>
      <p>Award: EDU2013-41974-P</p>
      <h2 class="heading">Notes</h2>
      <ol>
         <li>Proyecto para el desarrollo de Open Vídeo Anotaciones -OVA- para la plataforma MOOC de Edx (Monedero-Moya, Cebrian-Robles
            &amp; Desenne, 2015).
         </li>
         <li>We thank Professor Kurt E Fendt from MIT´s Digital Humanities Laboratory who allowed us to use the HyperStudio tool at Vidanet
            Project https://cutt.ly/tmhQmWd
         </li>
      </ol>
      <h1>REFERENCES</h1>
      <ol id="references">
         <li>Becker, K. (2010). The Clark-Kozma Debate in the 21st Century. In <i>CNIE 2010: Heritage Matters: Inspiring Tomorrow</i>.   Retrieved from <a href="https://core.ac.uk/download/pdf/51439897.pdf">https://core.ac.uk/download/pdf/51439897.pdf</a></li>
         
         <li>Bianco, S., Ciocca, G., Napoletano, P. &amp; Schettini, R. (2015). An interactive tool for manual, semi-automatic and automatic video annotation. <i>Computer Vision and Image Understanding</i>,  <i>131</i>, 88–99. <a href="https://doi.org/10.1016/j.cviu.2014.06.015">https://doi.org/10.1016/j.cviu.2014.06.015</a></li>
         
         <li>Buendía, L., Colás, P. &amp; Hernández, F. (1998).<i>Métodos de investigación en psicopedagogía</i>.McGraw-Hill
         </li>
         
         <li>Cebrián, D., Pérez, R. &amp; Cebrián, M. (2017). Estudio de la comunicación en la evaluación de los diarios de prácticas que favorecen la argumentación. <i>Revista Practicum</i>,  <i>2</i>(1), 1–21. Retrieved from <a href="https://revistas.uma.es/index.php/iop/article/view/8262">https://revistas.uma.es/index.php/iop/article/view/8262</a><a href="https://doi.org/10.24310/revpracticumrep.v2i1.8262">https://doi.org/10.24310/revpracticumrep.v2i1.8262</a></li>
         
         <li>Cebrián, M. (2017).<i>Recursos tecnológicos para un Ple-portafolios multimedia de calidad para el prácticum y las prácticas externas</i>. (pp. 34-48) Reppe  Retrieved from <a href="https://goo.gl/AbjWPz">https://goo.gl/AbjWPz</a></li>
         
         <li>Chen, L., Chen, G.-C., Xu, C.-Z., March, J. &amp; Benford, S. (2007). EmoPlayer: A media player for video clips with affective annotations. <i>Interacting with Computers</i>,  <i>20</i>(1), 17–28. <a href="https://doi.org/10.1016/j.intcom.2007.06.003">https://doi.org/10.1016/j.intcom.2007.06.003</a></li>
         
         <li>Colasante, M. (2011). Using video annotation to reflect on and evaluate physical education pre-service teaching practice. <i>Australasian Journal of Educational Technology</i>,  <i>27</i>(1), 27. <a href="https://doi.org/10.14742/ajet.983">https://doi.org/10.14742/ajet.983</a></li>
         
         <li>Debbag, M. &amp; Fidan, . M. (2020). Examination of Text and Video-Formatted Learning Diaries in the Teacher Education. <i>Australian Journal of Teacher Education</i>,  <i>45</i>(3), 1–17. <a href="https://doi.org/10.14221/ajte.2020v45n3.1">https://doi.org/10.14221/ajte.2020v45n3.1</a></li>
         
         <li>Dennen, V. P., Bagdy, L. M. &amp; Cates, M. L. (2018). Effective Tagging Practices for Online Learning Environments:  An Exploratory Study of Tag Approach and Accuracy. <i>Online Learning</i>,  <i>22</i>(3), 22. <a href="https://doi.org/10.24059/olj.v22i3.1471">https://doi.org/10.24059/olj.v22i3.1471</a></li>
         
         <li>Dias-Pereira-Dos-Santos, A., Loke, L. &amp; Martinez-Maldonado, R. (2018). Exploring Video Annotation as a Tool to Support Dance Teaching. In <i>Proceedings of the 30th Australian Conference on Computer-Human Interaction</i>. (pp. 448-452)  <a href="https://doi.org/10.1145/3292147.3292194">https://doi.org/10.1145/3292147.3292194</a></li>
         
         <li>Fundación Telefónica (2016). Top 100 - Innovaciones educativas 2016 Educar para la sociedad digital.  Retrieved from <a href="https://goo.gl/um5aWX">https://goo.gl/um5aWX</a></li>
         
         <li>Gao, F. (2013). A Case Study of Using a Social Annotation Tool to Support Collaboratively Learning. <i>The Internet and Higher Education</i>,  <i>17</i>, 76–83. <a href="https://doi.org/10.1016/j.iheduc.2012.11.002">https://doi.org/10.1016/j.iheduc.2012.11.002</a></li>
         
         <li>Gayoso-Cabada, J., Sarasa-Cabezuelo, A. &amp; Sierra-Rodríguez, J.-L. (2019). A review of annotation classification tools in the educational domain. <i>Open Computer Science</i>,  <i>9</i>(1), 299–307. <a href="https://doi.org/10.1515/comp-2019-0021">https://doi.org/10.1515/comp-2019-0021</a></li>
         
         <li>Godoy, D. &amp; Corbellini, A. (2016). Folksonomy Based Recommender Systems: A State of the Art Review. <i>International Journal of Intelligent Systems</i>,  <i>31</i>(4), 314–346. <a href="https://doi.org/10.1002/int.21753 ">https://doi.org/10.1002/int.21753 </a></li>
         
         <li>Hefter, M. H. &amp; Berthold, K. (2020). Preparing learners to self-explain video examples: Text or video introduction? <i>Computers in Human Behavior</i>,  <i>110</i>, 106404. <a href="https://doi.org/10.1016/j.chb.2020.106404">https://doi.org/10.1016/j.chb.2020.106404</a></li>
         
         <li>Hsu, I.-C. (2013). Integrating ontology technology with folksonomies for personalized social tag recommendation. <i>Applied Soft Computing</i>,  <i>13</i>(8), 3745–3750. <a href="https://doi.org/10.1016/j.asoc.2013.03.004">https://doi.org/10.1016/j.asoc.2013.03.004</a></li>
         
         <li>Imran, A. S., Cheikh, F. A. &amp; Kowalski, S. J. (2016). Automatic Annotation of Lecture Videos for Multimedia Driven Pedagogical Platforms. <i>Knowledge Management &amp; E-Learning: An International Journal</i>,  <i>8</i>(4), 550–580.
         </li>
         
         <li>Kandari, A., Qattan, A. M. A. &amp; M. M. (2020). E-Task-Based Learning Approach to Enhancing 21st-Century Learning Outcomes. <i>International Journal of Instructional Media</i>,  <i>13</i>(1), 551–566. <a href="https://doi.org/10.29333/iji.2020.13136a">https://doi.org/10.29333/iji.2020.13136a</a></li>
         
         <li>Lau, S. B. Y., Lee, C. S. &amp; Singh, Y. P. (2015). A Folksonomy-based Lightweight Resource Annotation Metadata Schema for Personalized Hypermedia Learning Resource Delivery.
            <i>Interactive Learning Environments</i>,  <i>23</i>(1), 79–105. <a href="https://doi.org/10.1080/10494820.2012.745429 ">https://doi.org/10.1080/10494820.2012.745429 </a></li>
         
         <li>Lee, H. Y. &amp; List, A. (2019). Processing of texts and videos: A strategy-focused analysis. <i>Journal of Computer Assisted Learning</i>,  <i>35</i>(2), 268–282. <a href="https://doi.org/10.1111/jcal.12328">https://doi.org/10.1111/jcal.12328</a></li>
         
         <li>List, A. &amp; Ballenger, E. E. (2019). Comprehension across mediums: the case of text and video. <i>Journal of Computing in Higher Education</i>,  <i>31</i>(3), 514–535. <a href="https://doi.org/10.1007/s12528-018-09204-9">https://doi.org/10.1007/s12528-018-09204-9</a></li>
         
         <li>List, A. (2018). Strategies for Comprehending and Integrating Texts and Videos. <i>Learning and Instruction</i>,  <i>57</i>, 34–46. <a href="https://doi.org/10.1016/j.learninstruc.2018.01.008 ">https://doi.org/10.1016/j.learninstruc.2018.01.008 </a></li>
         
         <li>Liu, F. &amp; Stapleton, P. (2014). Counterargumentation and the cultivation of critical thinking in argumentative writing: Investigating washback from a high-stakes
            test. <i>System</i>,  <i>45</i>, 117–128. <a href="https://doi.org/10.1016/j.system.2014.05.005">https://doi.org/10.1016/j.system.2014.05.005</a></li>
         
         <li>Martínez, J. D. G., Rigo, E. &amp; Jiménez, R. (2017). Multimedia and Textual Reading Comprehension: Multimedia as Personal Learning Environment’s Enriching Format. <i>Journal of New Approaches in Educational Research</i>,  <i>6</i>(1), 3–10. <a href="https://doi.org/10.7821/naer.2017.1.180">https://doi.org/10.7821/naer.2017.1.180</a></li>
         
         <li>McFadden, J., Ellis, J., Anwar, T. &amp; Roehrig, G. (2014). Beginning Science Teachers’ Use of a Digital Video Annotation Tool to Promote Reflective Practices. <i>Journal of Science Education and Technology</i>,  <i>23</i>(3), 458–470. <a href="https://doi.org/10.1007/s10956-013-9476-2">https://doi.org/10.1007/s10956-013-9476-2</a></li>
         
         <li>Mirriahi, N., Joksimović, S., Gašević, D. &amp; Dawson, S. (2018). Effects of instructional conditions and experience on student reflection: a video annotation study. <i>Higher Education Research &amp; Development</i>,  <i>37</i>(6), 1245–1259. <a href="https://doi.org/10.1080/07294360.2018.1473845">https://doi.org/10.1080/07294360.2018.1473845</a></li>
         
         <li>Mirriahi, N., Liaqat, D., Dawson, S. &amp; Gašević, D. (2016). Uncovering student learning profiles with a video annotation tool: reflective learning with and without instructional norms.
            <i>Educational Technology Research and Development</i>,  <i>64</i>(6), 1083–1106. <a href="https://doi.org/10.1007/s11423-016-9449-2">https://doi.org/10.1007/s11423-016-9449-2</a></li>
         
         <li>Monedero, J. J., Cebrián, D. &amp; Desenne, P. (2015). Usability and Satisfaction in Multimedia Annotation Tools for MOOCs. <i>Comunicar</i>,  <i>22</i>(44), 55–62. <a href="https://doi.org/10.3916/c44-2015-06">https://doi.org/10.3916/c44-2015-06</a></li>
         
         <li>Muellner, L. (2015). Annotations and the Ancient Greek Hero: Past, Present, and Future. <i>Comunicar</i>,  <i>22</i>(44), 45–53. <a href="https://doi.org/10.3916/c44-2015-05">https://doi.org/10.3916/c44-2015-05</a></li>
         
         <li>Novak, E., Razzouk, R. &amp; Johnson, T. E. (2012). The educational use of social annotation tools in higher education: A literature review. <i>The Internet and Higher Education</i>,  <i>15</i>(1), 39–49. <a href="https://doi.org/10.1016/j.iheduc.2011.09.002">https://doi.org/10.1016/j.iheduc.2011.09.002</a></li>
         
         <li>Nussbaum, E. M., Sinatra, G. M. &amp; Poliquin, A. (2008). Role of Epistemic Beliefs and Scientific Argumentation in Science Learning. <i>International Journal of Science Education</i>,  <i>30</i>(15), 1977–1999. <a href="https://doi.org/10.1080/09500690701545919">https://doi.org/10.1080/09500690701545919</a></li>
         
         <li>Özçinar, H. (2015). Scaffolding computer-mediated discussion to enhance moral reasoning and argumentation quality in pre-service teachers. <i>Journal of Moral Education</i>,  <i>44</i>(2), 232–251. <a href="https://doi.org/10.1080/03057240.2015.1043875">https://doi.org/10.1080/03057240.2015.1043875</a></li>
         
         <li>Paradis, J. &amp; Fendt, K. E. (2016). Annotation Studio: Digital Annotation as an Educational Approach in the Humanities and Arts, White Paper. Massachusetts Institute of Technology.  Retrieved from <a href="https://cutt.ly/TfacQEC">https://cutt.ly/TfacQEC</a></li>
         
         <li>Pérez-Torregrosa, A. B., Díaz-Martín, C. &amp; Ibáñez-Cubillas, P. (2017). The Use of Video Annotation Tools in Teacher Training. <i>Procedia - Social and Behavioral Sciences</i>,  <i>237</i>, 458–464. <a href="https://doi.org/10.1016/j.sbspro.2017.02.090">https://doi.org/10.1016/j.sbspro.2017.02.090</a></li>
         
         <li>Po-Sheng, C., Hsin-Chin, C., Yueh-Min, H., Chia-Ju, L., Ming-Chi, L. &amp; Ming-Hsun, S. (2018). A Video Annotation Learning Approach to Improve the Effects of Video Learning. <i>Innovations in Education and Teaching International</i>,  <i>55</i>(4), 459–469. <a href="https://doi.org/10.1080/14703297.2016.1213653 ">https://doi.org/10.1080/14703297.2016.1213653 </a></li>
         
         <li>Qarabash, H., Heslop, P., Kharrufa, A., Balaam, M. &amp; Devlin, M. (2019). Group tagging: Using video tagging to facilitate reflection on small group activities. <i>British Journal of Educational Technology</i>,  <i>50</i>(4), 1913–1928. <a href="https://doi.org/10.1111/bjet.12691">https://doi.org/10.1111/bjet.12691</a></li>
         
         <li>Sauli, F., Cattaneo, A. &amp; Meij, H. van der (2018). Hypervideo for educational purposes: a literature review on a multifaceted technological tool. <i>Technology, Pedagogy and Education</i>,  <i>27</i>(1), 115–134. <a href="https://doi.org/10.1080/1475939x.2017.1407357">https://doi.org/10.1080/1475939x.2017.1407357</a></li>
         
         <li>Smith, B.K., Blankinship, E. &amp; Lackner, T. (2000). Annotation and education. <i>IEEE Multimedia</i>,  <i>7</i>(2), 84–89. <a href="https://doi.org/10.1109/93.848437">https://doi.org/10.1109/93.848437</a></li>
         
         <li>Su, A. Y. S., Yang, S. J. H., Hwang, W. Y. &amp; Zhang, J. (2010). A Web 2.0-based Collaborative Annotation System for Enhancing Knowledge Sharing in Collaborative Learning Environments. <i>Computers &amp; Education</i>,  <i>55</i>(2), 752–766. <a href="https://doi.org/10.1016/j.compedu.2010.03.008">https://doi.org/10.1016/j.compedu.2010.03.008</a></li>
         
         <li>Sydnor, J. (2016). Using Video to Enhance Reflective Practice: Student Teachers’ Dialogic Examination of Their Own Teaching. <i>The New Educator</i>,  <i>12</i>(1), 67–84. <a href="https://doi.org/10.1080/1547688x.2015.1113346">https://doi.org/10.1080/1547688x.2015.1113346</a></li>
         
         <li>Zhu, X., Chen, B., Avadhanam, R., Shui, H. &amp; Zhang, R. (2020). Reading and Connecting: Using Social Annotation in Online Classes.     Retrieved from <a href="https://doi.org/10.35542/osf.io/2nmxp">https://doi.org/10.35542/osf.io/2nmxp</a></li>
         
      </ol>
   </article>
</body>